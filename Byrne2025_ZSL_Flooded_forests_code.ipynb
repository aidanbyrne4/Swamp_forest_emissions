{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1QpO4cwxtr5a9GL-_EtGyBgDfXYGON-RT","timestamp":1748867871183},{"file_id":"1KUQOzM-K_IjWYwQ8eSCqCg420afW65XJ","timestamp":1748851962510},{"file_id":"1ln9NY03r9tIf2OevRq1vfYk8vloXpZg4","timestamp":1748347457856}],"toc_visible":true,"collapsed_sections":["1e9G85RXul6N","uHs-HLBvuKA_","DBmoqwq92Jui","2FJH4yp4uQRN","yesRcN0mpnq7","FwkJpvaSiRjc","H-9fY5N4iYir","-Gpr-Jnniufc","M_SNvePui5Jc","mWfcf0paiYqk","m1SF3o7WjfEU","L6FJlEwEkzfh","88ncJ2Lp8ElO","WD21U2ZsvoNm","PBdWYOKzvrZ5","um6m6W3IvwVg"],"authorship_tag":"ABX9TyN6HIMyCbiLHsLsaGE4ON0R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Quantifying swamp forest extent changes and wetland Greenhouse Gas (GHG) emissions in the Congo Basin\n","\n","Author: Aidan Byrne - aidan.byrne@ioz.ac.uk  \n","Script created: 12/03/2025  \n","Last modified: 09/06/2025  \n","\n","Processes PALSAR imagery and ancilliary datasets from Google Earth Engine (GEE) to classify swamp forest extents, forest productivity, water table levels and GHG emissions in the Congo basin from 2007 to 2024. *External data required to run the script can be downloaded from https://github.com/aidanbyrne4/Swamp_forest_emissions\n","\n","Requirements:\n","1. Google drive access\n","2. Google earth engine access\n","3. 2 X Empty Google Earth Engine Image Collection assets\n","\n","Inputs:\n","1.   GEE data (PALSAR, MODIS EVI, MERIT Hydro, Forest cover)\n","2.   Groundtruth coordinates for classification (requires download)*\n","3.   GHG fluxes by water table level (requires download)*\n","\n","Outputs:\n","1. Annual swamp forest extents in the Congo Basin\n","2. Annual EVI values for swamp forests, dry forests and all forests\n","3. Annual wetland fluxes of CO<sub>2</sub>, CH<sub>4</sub> and N<sub>2</sub>O and CO<sub>2</sub>-equivalent\n","\n","NOTE: Classification accuracy and the subsequent analyses are subject to minor variations due to upstream source data and code functionality updates on the Google Earth Engine servers."],"metadata":{"id":"KS2zj_2FusWZ"}},{"cell_type":"markdown","source":["# Load packages"],"metadata":{"id":"1e9G85RXul6N"}},{"cell_type":"code","source":["!pip install scikit-posthocs\n","!pip install rasterio"],"metadata":{"id":"hg_mmSWhIXhO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import ee\n","from google.colab import drive\n","import math\n","import numpy as np\n","import pandas as pd\n","import geemap\n","import statsmodels.api as sm\n","import matplotlib.pyplot as plt\n","import matplotlib.dates as mdates\n","import rasterio\n","from rasterio.merge import merge\n","from rasterio.plot import show\n","import os\n","from sklearn.linear_model import LinearRegression\n","from scipy import stats\n","from scipy.stats import ttest_rel, wilcoxon\n","from scipy.stats import linregress, t"],"metadata":{"id":"6GwXWFTv3Qcw","executionInfo":{"status":"ok","timestamp":1749463918608,"user_tz":-60,"elapsed":3391,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Define GEE project and Google drive (required)"],"metadata":{"id":"lFfC15TW--3u"}},{"cell_type":"code","source":["### Define the destination to export PALSAR classification and EVI images - your empty GEE image collection assets (must be an image collection, not a folder)\n","# you must create this in the GEE code editor first and ensure the asset names match the following varibales\n","GEE_project = 'ee-YOUR_USERNAME' # your Google Earth Engine cloud project username e.g. 'ee-JohnSmith'\n","GEE_ImageCollection_PALSAR = 'YOUR_PALSAR_IMAGE_COLLECTION' # name of your empty GEE PALSAR image collection asset\n","GEE_ImageCollection_EVI = 'YOUR_EVI_IMAGE_COLLECTION' # name of your empty GEE EVI image collection asset\n","\n","# google drive folder and path for data exports/loading\n","GoogleDrive_folder = 'MyDrive' # google drive folder where you have saved the downloaded data (groundtruth and GHG flux data) - stats exports will go here (defaults to MyDrive)\n","GoogleDrive_folder_path = f'/content/drive/{GoogleDrive_folder}' # if you change from the default folder 'MyDrive', you will need to update the folder path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"5jLKAsK__EFO","executionInfo":{"status":"ok","timestamp":1749463986189,"user_tz":-60,"elapsed":40,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"08b54602-179b-49d3-f22b-9b387ae4b11c"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"markdown","source":["# Load GEE and mount drive"],"metadata":{"id":"uHs-HLBvuKA_"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"mzyAjDyquDNI","executionInfo":{"status":"ok","timestamp":1749463989012,"user_tz":-60,"elapsed":1432,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"9568f873-7be5-48f9-ac72-8864dfc6413a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}],"source":["### Connect to Google Earth Engine\n","# Trigger the authentication flow\n","ee.Authenticate()\n","\n","# Initialize the library - your cloud username\n","ee.Initialize(project=GEE_project)"]},{"cell_type":"code","source":["### Connect to google drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"AX-pBgyquIle","executionInfo":{"status":"ok","timestamp":1749463994413,"user_tz":-60,"elapsed":3255,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"960127cc-fbe1-4c38-a4ab-79728c9d097c"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Define parameters"],"metadata":{"id":"DBmoqwq92Jui"}},{"cell_type":"code","source":["### Leave unchanged\n","# asset paths for exporting and loading datasets\n","# asset_path_PALSAR = 'projects/ee-aidanbyrne/assets/YOUR_PALSAR_IMAGE_COLLECTION/'\n","# asset_path_EVI = 'projects/ee-aidanbyrne/assets/YOUR_EVI_IMAGE_COLLECTION/'\n","asset_path_PALSAR = f'projects/{GEE_project}/assets/{GEE_ImageCollection_PALSAR}/'\n","asset_path_PALSAR_loadData = f'projects/{GEE_project}/assets/{GEE_ImageCollection_PALSAR}'\n","asset_path_EVI = f'projects/{GEE_project}/assets/{GEE_ImageCollection_EVI}/'\n","asset_path_EVI_loadData = f'projects/{GEE_project}/assets/{GEE_ImageCollection_EVI}'\n","\n","# Classification image date\n","startDateImage = ee.Date('2019-01-01')\n","endDateImage = ee.Date('2020-01-01')\n","\n","# study period for timeseries\n","startDate = ee.Date('2007-01-01')\n","endDate = ee.Date('2025-01-01')\n","\n","# study period for timeseries - PALSAR-1\n","startDatePalsar1 = ee.Date('2007-01-01')\n","endDatePalsar1 = ee.Date('2011-01-01')\n","\n","# study period for timeseries - PALSAR-2\n","startDatePalsar2 = ee.Date('2015-01-01')\n","endDatePalsar2 = ee.Date('2025-01-01')\n","\n","# Spatial scale for exporting area and EVI calculations (reduce region function)\n","spatialScale = 90 # to match classifications with HAND resolution for flood depths and WTLs\n","tileScale = 2 # to split calculations over subsections when exporting - avoids exceeding memory usage"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"u7-yO4XR3n2i","executionInfo":{"status":"ok","timestamp":1749464014565,"user_tz":-60,"elapsed":46,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"e352b530-6e8a-4bf3-d996-133bd044b347"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"markdown","source":["# Load data"],"metadata":{"id":"2FJH4yp4uQRN"}},{"cell_type":"code","source":["### Define study area, datasets, and static layers\n","\n","# Get the Congo basin geometry from the HydroSHEDS collection\n","basin = (ee.FeatureCollection('WWF/HydroSHEDS/v1/Basins/hybas_3')\n","         .filter(ee.Filter.eq('HYBAS_ID', 1030020040))).first().geometry()\n","\n","basinSimple = basin.simplify(500) # smooths polygon vertices to reduce processing time when exporting data\n","\n","# select subbasins\n","basinfilter = basin.buffer(-100)\n","\n","subbasins = (ee.FeatureCollection('WWF/HydroSHEDS/v1/Basins/hybas_4')\n","             .filterBounds(basinfilter))\n","\n","# DEM\n","meritDEM = ee.Image(\"MERIT/DEM/v1_0_3\").clip(basin)\n","\n","# Height Above Nearest Drainage (HAND)\n","HAND = ee.Image('MERIT/Hydro/v1_0_1').clip(basin)\n","\n","# Forest layer\n","forest = ee.ImageCollection('UMD/GLAD/PRIMARY_HUMID_TROPICAL_FORESTS/v1').mosaic().selfMask().clip(basin) # new data with better map\n","forest = forest.select(\"Primary_HT_forests\").rename(\"Map\")\n","forestBinary = forest.select('Map').eq(1) # for spatial trends\n","forestMap = forestBinary.selfMask().unmask(0).clip(basin) # for spatial trends\n","\n","# Rivers\n","rivers = ee.Image('MERIT/Hydro/v1_0_1').select('wat').eq(1).clip(basin)\n","rivers = rivers.updateMask(rivers.gte(0.4))\n","riversMask = rivers.unmask(0).clip(basin)\n","openwater = ee.ImageCollection(\"GLCF/GLS_WATER\").mosaic().select('water').eq(2).clip(basin)\n","\n","# PALSAR-2\n","PALSARCollection = (ee.ImageCollection('JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR')\n","                     .filterBounds(basin)\n","                     .filter(ee.Filter.eq('PassDirection', 'Descending')))\n","# PALSAR-1\n","PALSARCollectionannual = (ee.ImageCollection('JAXA/ALOS/PALSAR/YEARLY/SAR_EPOCH'))\n","                     #.filterBounds(basin)\n","                     #.filter(ee.Filter.eq('PassDirection', 'Descending')))\n","\n","# MODIS EVI\n","modis = ee.ImageCollection('MODIS/061/MOD13A3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"iDc3dY3Z3lBF","executionInfo":{"status":"ok","timestamp":1749464016508,"user_tz":-60,"elapsed":11,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"e6bfe8c0-f626-42de-93eb-e53c349cb0ca"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"markdown","source":["# Define functions"],"metadata":{"id":"yesRcN0mpnq7"}},{"cell_type":"markdown","source":["## Preprocess GEE data"],"metadata":{"id":"FwkJpvaSiRjc"}},{"cell_type":"code","source":["#######################\n","###### PALSAR-2 #######\n","#######################\n","\n","def applyBitmaskPalsar2(image):\n","    msk = image.select('MSK')\n","    quality = msk.bitwiseAnd(7)  # extract bits 0-2\n","    validMask = quality.eq(1)\n","    return image.updateMask(validMask)\n","\n","def preprocessPalsarPalsar2(image):\n","    \"\"\"Convert DN to backscatter in dB for PALSAR image.\"\"\"\n","    hh = image.select('HH').pow(2).log10().multiply(10).add(-83.0)\n","    hv = image.select('HV').pow(2).log10().multiply(10).add(-83.0)\n","    lin = image.select('LIN').multiply(0.01)\n","    filtered = hh.addBands(hv).addBands(lin)\n","    return filtered\n","\n","def correctIncidenceAnglePalsar2(image):\n","    localAngle = image.select('LIN').multiply(math.pi / 180)\n","    refAngle = ee.Image.constant(37).multiply(math.pi / 180)\n","    hhLinear = ee.Image(10).pow(image.select('HH').divide(10.0))\n","    hvLinear = ee.Image(10).pow(image.select('HV').divide(10.0))\n","    hhCorrected = hhLinear.multiply(refAngle.cos()).divide(localAngle.cos()).rename('HH_corrected')\n","    hvCorrected = hvLinear.multiply(refAngle.cos()).divide(localAngle.cos()).rename('HV_corrected')\n","    hhCorrectedDb = hhCorrected.log10().multiply(10).rename('HH_corrected_db')\n","    hvCorrectedDb = hvCorrected.log10().multiply(10).rename('HV_corrected_db')\n","    return image.addBands(hhCorrectedDb).addBands(hvCorrectedDb)\n","\n","def leeFilter(image):\n","    kernel = ee.Kernel.square(radius=1, units='pixels', normalize=False)\n","    # Compute the local mean and variance using the 3x3 kernel\n","    localMean = image.reduceNeighborhood(reducer=ee.Reducer.mean(), kernel=kernel)\n","    localVariance = image.reduceNeighborhood(reducer=ee.Reducer.variance(), kernel=kernel)\n","    # Compute the weight for each pixel\n","    weight = localVariance.subtract(localMean).divide(localVariance).max(0)\n","    # Apply the filter to all bands using the calculated weight\n","    filtered = localMean.add(weight.multiply(image.subtract(localMean)))\n","    # Ensure that the band names remain unchanged\n","    filtered = filtered.rename(image.bandNames())\n","    return filtered\n","\n","# apply speckle filter (3x3) to classification layer to remove isolated pixels for further WTLs analyses\n","def kernelFilter(image):\n","  square_kernel = ee.Kernel.square(radius=1, units='pixels') #radius 1 for 3x3 kernel, 2 for 5x5 kernel, 4 for figures smoother\n","  classification = image.select(\"flooded_classification\") \\\n","                  .focal_mode(kernel=square_kernel, iterations=1).rename(\"flooded_classification_kernel\")\n","  return image.addBands(classification)\n","\n","######################\n","##### Palsar 1 #######\n","######################\n","\n","def applyBitmaskPalsar1(image):\n","    \"\"\"Apply QA band mask to remove bad data (No data, Layover, Shadowing).\"\"\"\n","    # Select the QA band\n","    qa = image.select('qa')\n","    # Create a mask where QA values are not in [0, 100, 150]\n","    mask = qa.neq(0).And(qa.neq(100)).And(qa.neq(150))\n","    return image.updateMask(mask)\n","\n","### for gap filling 2008 image\n","def createNoDataMaskPalsar1(image):\n","    \"\"\"Create a binary image where 1 = No data (qa == 0), 0 = Data pixels.\"\"\"\n","    # Select the QA band\n","    qa = image.select('qa')\n","    # Create a binary mask: 1 where qa == 0 (No data), 0 otherwise\n","    no_data_mask = qa.eq(0).rename('no_data_mask')\n","    return no_data_mask\n","\n","def preprocessPalsarPalsar1(image):\n","    \"\"\"Preprocess PALSAR HH and HV bands to dB and retain incidence angle.\"\"\"\n","    # Convert HH and HV bands to dB scale\n","    hh = image.select('HH').pow(2).log10().multiply(10).add(-83.0)\n","    hv = image.select('HV').pow(2).log10().multiply(10).add(-83.0)\n","    # Select incidence angle (no modification needed here)\n","    lin = image.select('angle')\n","    # Remove the original HH and HV bands and add the processed ones\n","    processed = image.select(['angle']) \\\n","                     .addBands(hh.rename('HH')) \\\n","                     .addBands(hv.rename('HV'))  # Retain 'angle' and replace 'HH' and 'HV' bands\n","    # Return the processed image with replaced HH and HV bands\n","    return processed\n","\n","# Fill data gaps in 2008 annual image using 2007 image\n","def Fill_data_gaps_PALSAR1(image):\n","    # Compare the image ID to that of the 2008 image\n","    cond = ee.String(image.id()).equals(ee.String(PALSARimage2008.id()))\n","    # Use ee.Algorithms.If to choose between the updated image and the original image\n","    return ee.Image(ee.Algorithms.If(cond, PALSARimage2008Updated, image))\n","\n","def maskModisQA(image):\n","    qa = image.select('SummaryQA')\n","    # Create a mask: True where QA equals 0, False elsewhere.\n","    mask = qa.lte(1)\n","    # Update the image mask; only good pixels will be kept.\n","    return image.updateMask(mask)\n","\n","def processEVI(image):\n","    return image.select('EVI').multiply(0.0001) # apply scaling factor"],"metadata":{"id":"Fo3ObvK7ikxU","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1749464018029,"user_tz":-60,"elapsed":22,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"1447067e-bc74-4f91-a038-6217aa1dc6cb"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"markdown","source":["## Preprocess groundtruth data"],"metadata":{"id":"H-9fY5N4iYir"}},{"cell_type":"code","source":["######################################\n","#### ground truth data processing ####\n","######################################\n","\n","def df_to_ee_feature_collection(df):\n","    # Create a list to store features\n","    features = []\n","    # Iterate through each row in the dataframe\n","    for index, row in df.iterrows():\n","        # Create a point geometry using lat and lon\n","        point = ee.Geometry.Point([float(row['Lon']), float(row['Lat'])])\n","        # Create properties dictionary with Landcover_classification\n","        properties = {\n","            'Landcover_classification': row['Landcover_classification']\n","        }\n","        # Create feature with geometry and properties\n","        feature = ee.Feature(point, properties)\n","        features.append(feature)\n","    return ee.FeatureCollection(features)\n","\n","# Map string labels to numerical values for classification\n","def add_class_property(feature):\n","    class_value = feature.getString('Landcover_classification')\n","    class_num = ee.Algorithms.If(class_value.equals('Peat swamp forest'), 1, 0)\n","    return feature.set('class', class_num)"],"metadata":{"id":"EWoaKqp-ilLU","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1749464018031,"user_tz":-60,"elapsed":1,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"8c25ffd8-07e4-4391-f1be-b428da9667f6"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"markdown","source":["## Classify swamp forests"],"metadata":{"id":"-Gpr-Jnniufc"}},{"cell_type":"code","source":["######################################\n","#### classify swamp forest extent ####\n","######################################\n","\n","### palsar 2\n","\n","def getFloodedImage(interval):\n","    \"\"\"For a given interval dictionary, return a image with classified swamp forest pixels\"\"\"\n","    interval = ee.Dictionary(interval)\n","    s = ee.Date(interval.get('start'))\n","    e = ee.Date(interval.get('end'))\n","    # Filter PALSAR images for the given time interval and process\n","    palsarInterval = PALSARCollection.filterDate(s, e)\n","    processedPalsar = (palsarInterval\n","                       .map(applyBitmaskPalsar2)\n","                       .map(leeFilter)\n","                       .map(preprocessPalsarPalsar2)\n","                       .map(correctIncidenceAnglePalsar2))\n","    palsarImage = processedPalsar.median().clip(basin)\n","    # Create the HV/HH ratio band.\n","    ratio = palsarImage.addBands(\n","        palsarImage.select('HV_corrected_db')\n","                   .divide(palsarImage.select('HH_corrected_db'))\n","                   .rename('HV_HH_ratio')\n","    )\n","    # Create forest binary mask and apply HAND threshold.\n","    HANDlt20 = HAND.select('hnd').lt(20)\n","    forestBinary = forest.select('Map').eq(1)\n","    forestBinary20 = forestBinary.updateMask(HANDlt20)\n","    HANDlt20binary = HANDlt20.eq(1)\n","    flooded20 = HANDlt20binary.multiply(forestBinary20)\n","    flooded20 = flooded20.updateMask(flooded20.connectedPixelCount(8).gt(5))\n","    DEMthreshold = meritDEM.select('dem').lte(600)\n","\n","    # classify the image.\n","    combined_image = ratio.addBands(HAND.select('hnd'))\n","    # Apply the masks: use the flooded mask and DEM threshold\n","    image_to_classify = combined_image.updateMask(flooded20).updateMask(DEMthreshold)\n","    # Apply the classifier\n","    classified = image_to_classify.classify(classifier).rename('flooded_classification')\n","    # unmask non-flooded pixels for further analyses\n","    classified = classified.unmask(0).clip(basin)\n","    #forestMap = forestBinary.selfMask().unmask(0)\n","    # Combine the bands into one image - only one band for PALSAR-2\n","    image = ee.Image.cat([\n","        classified\n","    ])\n","    # Set image properties with the start date\n","    image = image.set('system:time_start', s.millis()).set('date', s.format('YYYY-MM-dd')).set('year', s.format('YYYY'))\n","    return image\n","\n","#### palsar 1\n","\n","def getFloodedImageFromMosaic(interval):\n","    \"\"\"For a given interval dictionary, compute and return an image with several bands.\"\"\"\n","    interval = ee.Dictionary(interval)\n","    s = ee.Date(interval.get('start'))\n","    e = ee.Date(interval.get('end'))\n","    # Filter the annual mosaic for the given time interval and process\n","    annualMosaicInterval = PALSARCollectionannual.filterDate(s, e).first()\n","    # create no data mask for gap filling\n","    no_data_band = createNoDataMaskPalsar1(annualMosaicInterval).clip(basin).unmask(1)\n","    # Process the image\n","    processedPalsar = applyBitmaskPalsar1(annualMosaicInterval)\n","    processedPalsar = leeFilter(processedPalsar)\n","    processedPalsar = preprocessPalsarPalsar1(processedPalsar)\n","    mosaicImage = processedPalsar.clip(basin)\n","\n","    # Create the HV/HH ratio band\n","    ratio = mosaicImage.addBands(\n","        mosaicImage.select('HV').divide(mosaicImage.select('HH')).rename('HV_HH_ratio')\n","    )\n","    # rename bands to match classifier for palsar-2\n","    ratio = ratio.select([\"HH\", \"HV\", \"HV_HH_ratio\"]).rename([\"HH_corrected_db\", \"HV_corrected_db\", \"HV_HH_ratio\"])\n","    # Create forest binary mask and apply HAND and DEM threshold to identify potential flooding area\n","    HANDlt20 = HAND.select('hnd').lt(20)\n","    forestBinary = forest.select('Map').eq(1)\n","    forestBinary20 = forestBinary.updateMask(HANDlt20)\n","    HANDlt20binary = HANDlt20.eq(1)\n","    flooded20 = HANDlt20binary.multiply(forestBinary20)\n","    flooded20 = flooded20.updateMask(flooded20.connectedPixelCount(8).gt(5))\n","    DEMthreshold = meritDEM.select('dem').lte(600)\n","\n","    # classify the image.\n","    combined_image = ratio.addBands(HAND.select('hnd'))\n","    # Apply the masks: use the flooded mask and DEM threshold\n","    image_to_classify = combined_image.updateMask(flooded20).updateMask(DEMthreshold)\n","    # Apply the classifier\n","    classified = image_to_classify.classify(classifier).rename('flooded_classification')\n","    # unmask pixels for further analyses\n","    classified = classified.unmask(0).clip(basin)\n","    #forestMap = forestBinary.selfMask().unmask(0)\n","    # Combine the bands into one image\n","    image = ee.Image.cat([\n","        classified,          # from peat swamp RF classification\n","        no_data_band         # no data mask\n","    ])\n","    # Set image properties with the start date\n","    image = image.set('system:time_start', s.millis()).set('date', s.format('YYYY-MM-dd'))\n","    return image"],"metadata":{"id":"57xFtx6Pi29c","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1749464018084,"user_tz":-60,"elapsed":52,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"c4740ee1-99fa-4c39-9069-7596e831251e"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"markdown","source":["## Export data"],"metadata":{"id":"4g215RzpjHe0"}},{"cell_type":"code","source":["########################\n","##### export data ######\n","########################\n","\n","### reduce bands in data exports\n","def select_bands_PALSAR1(image):\n","    return image.select([\"flooded_classification\", \"no_data_mask\"]) #palsar1\n","\n","def select_bands_PALSAR2(image):\n","    return image.select([\"flooded_classification\"]) #palsar2\n","\n","def select_bands_EVI(image):\n","    return image.select([\"EVI_mean\"]) #evi\n","\n","def set_year_property(image):\n","    # Extract the year from the system:time_start property\n","    date = ee.Date(image.get('system:time_start'))\n","    year = date.get('year')  # returns an integer (server-side)\n","    return image.set('year', year)\n","\n","### resample and export PALSAR-1 and PALSAR-2 classified images to GEE image collection asset for further analyses\n","def save_PALSAR2_to_asset(image, index):\n","    # Extract the year from the system:time_start property\n","    date = ee.Date(image.get('system:time_start'))\n","    year = date.format('YYYY') # timeseries\n","    # Create a unique asset name using the year\n","    asset_name = f\"{asset_path_PALSAR}Annual_mean_classified_flood_{year.getInfo()}\" # works for timeseries\n","    # # Define a fixed projection for consistency between datasets and compatibility with EVI\n","    fixed_projection = ee.Projection('EPSG:4326').atScale(90) # 90m pixels to match HAND resolution for flood depths and WTLs\n","    # Reproject\n","    image_reprojected = image.setDefaultProjection(fixed_projection)\n","    # Apply mode reducer to flooded classification layer (binary)\n","    flood_class = image_reprojected.select('flooded_classification') \\\n","        .reduceResolution(reducer=ee.Reducer.mode(), bestEffort=True)\n","    # Combine reduced layers back into one image\n","    reduced_image = flood_class#.addBands(nodata_resampled)\n","    # Export the image as an asset to GEE\n","    export_task = ee.batch.Export.image.toAsset(\n","        image=reduced_image,\n","        description=f\"export_image_{index}\",\n","        assetId=asset_name,\n","        scale=90,\n","        region=image.geometry(),\n","        maxPixels=1e13\n","    )\n","    # Start the export task\n","    export_task.start()\n","    print(f\"Exporting image {index} with year {year.getInfo()} to {asset_name}\") # timeseries\n","\n","def save_PALSAR1_to_asset(image, index):\n","    # Extract the year from the system:time_start property\n","    date = ee.Date(image.get('system:time_start'))\n","    year = date.format('YYYY') # timeseries\n","    # Create a unique asset name using the year\n","    asset_name = f\"{asset_path_PALSAR}Annual_mean_classified_flood_{year.getInfo()}\" # works for timeseries\n","    # # Define a fixed projection for consistency between datasets and compatibility with EVI\n","    fixed_projection = ee.Projection('EPSG:4326').atScale(90)\n","    # Reproject\n","    image_reprojected = image.setDefaultProjection(fixed_projection)\n","    # Apply mode reducer to flooded classification layer (binary)\n","    flood_class = image_reprojected.select('flooded_classification') \\\n","        .reduceResolution(reducer=ee.Reducer.mode(), bestEffort=True)\n","    # Apply mode reducer to no data mask (binary)\n","    nodata_resampled = image_reprojected.select('no_data_mask') \\\n","        .reduceResolution(reducer=ee.Reducer.mode(), bestEffort=True)\n","    # Combine reduced layers back into one image\n","    reduced_image = flood_class.addBands(nodata_resampled)\n","    # Export the image as an asset\n","    export_task = ee.batch.Export.image.toAsset(\n","        image=reduced_image,\n","        description=f\"export_image_{index}\",\n","        assetId=asset_name,\n","        scale=90,\n","        region=image.geometry(),\n","        maxPixels=1e13\n","    )\n","    # Start the export task\n","    export_task.start()\n","    print(f\"Exporting image {index} with year {year.getInfo()} to {asset_name}\") # timeseries\n","\n","### Extract the year from the EVI image date, resample and save it as a GEE image collection asset\n","def save_EVI_to_asset(image, index):\n","    # Extract the year from the system:time_start property\n","    date = ee.Date(image.get('system:time_start'))\n","    year = date.format('YYYY') # timeseries\n","    # Create a unique asset name using the year\n","    asset_name = f\"{asset_path_EVI}Annual_mean_classified_flood_{year.getInfo()}\" # works for timeseries\n","    # # Define a fixed projection for consistency between datasets and compatibility with EVI\n","    fixed_projection = ee.Projection('EPSG:4326').atScale(1000) # default EVI pixel reesolution\n","    # Reproject to match palsar projection\n","    #image_reprojected = image.setDefaultProjection(fixed_projection)\n","    image_reprojected = image.reproject(fixed_projection)\n","    # Apply to mean EVI layer (continuous)\n","    eviMean_resampled = image_reprojected.select('EVI_mean') \\\n","        .reduceResolution(reducer=ee.Reducer.mean(), bestEffort=True)\n","    # Combine reduced layers back into one image\n","    reduced_image = eviMean_resampled#.addBands(eviMax_resampled).addBands(eviMin_resampled) #evi\n","    # Export the image as an asset\n","    export_task = ee.batch.Export.image.toAsset(\n","        image=reduced_image,\n","        description=f\"export_image_{index}\",\n","        assetId=asset_name,\n","        scale=1000,\n","        region=image.geometry(),\n","        maxPixels=1e13\n","    )\n","    # Start the export task\n","    export_task.start()\n","    print(f\"Exporting image {index} with year {year.getInfo()} to {asset_name}\") # timeseries\n","\n","### Extract timeseries for subbasins swamp forest extents\n","def getFloodedAreaSubbasins(image):\n","    # Extract year from image date\n","    date = ee.Date(image.get('system:time_start'))\n","    year = date.format('YYYY')\n","    # Create masked images for each classification\n","    class_mask = image.select('flooded_classification').eq(1)\n","    #class_kernel_mask = image.select('flooded_classification_kernel').eq(1)\n","    # Create pixel area images\n","    area_class = ee.Image.pixelArea().updateMask(class_mask).rename('flooded_classified_area')\n","    #area_kernel = ee.Image.pixelArea().updateMask(class_kernel_mask).rename('flooded_kernel_area')\n","    # Combine both into one image\n","    area_image = area_class#.addBands(area_kernel)\n","    # Sum pixel areas over subbasins to get total area\n","    reduced = area_image.reduceRegions(\n","        collection=subbasins_forest,\n","        reducer=ee.Reducer.sum(),\n","        scale=spatialScale,\n","        tileScale=tileScale\n","    )\n","    # Attach year, convert m² to km², and retain HYBAS_ID #### failed after 52 minutes\n","    def format_feature(feature):\n","        class_m2 = feature.get('flooded_classified_area')\n","        #kernel_m2 = feature.get('flooded_kernel_area')\n","        return feature.set({\n","            'year': year,\n","            'HYBAS_ID': feature.get('HYBAS_ID'),\n","            'flooded_area_km2_classified': ee.Number(class_m2),#.divide(1e6),\n","            #'flooded_area_km2_classified_kernel': ee.Number(kernel_m2).divide(1e6)\n","        })\n","    return reduced.map(format_feature)\n","\n","# Compute flooded area per year and mean EVI in forests for final data export\n","def getFloodedAreaAndEVI(image):\n","    \"\"\"Compute flooded forest area for a given PALSAR image.\"\"\"\n","    date = ee.Date(image.get('system:time_start'))\n","    year = date.format('YYYY')\n","    # select bands for area and mean calculations\n","    classified = image.select('flooded_classification').eq(1)\n","    evi_forests = image.select('EVI_mean').updateMask(forest)\n","    evi_flooded = image.select('EVI_mean').updateMask(always_flooded)\n","    evi_nonflooded = evi_forests.updateMask(dry_forests)\n","    # Calculate flooded area in km² for classification method\n","    pixelAreaMaskedclass = ee.Image.pixelArea().updateMask(classified)\n","    floodedAreaclass = pixelAreaMaskedclass.reduceRegion(\n","        reducer=ee.Reducer.sum(),\n","        geometry=basinSimple,\n","        scale=spatialScale,\n","        maxPixels=1e13,\n","        bestEffort=True,\n","        tileScale=tileScale\n","    )\n","    areaValueclass = ee.Number(floodedAreaclass.get('area')).divide(1e6)\n","\n","    # Calculate mean EVI in all forests\n","    eviMeanForests = evi_forests.reduceRegion(\n","        reducer=ee.Reducer.mean(),\n","        geometry=basinSimple,\n","        scale=spatialScale,\n","        maxPixels=1e13,\n","        bestEffort=True,\n","        tileScale=tileScale\n","    )\n","    eviMeanForestsOutput = ee.Number(eviMeanForests.get('EVI_mean'))\n","\n","    # Calculate mean EVI in flooded forests\n","    eviMeanFlooded = evi_flooded.reduceRegion(\n","        reducer=ee.Reducer.mean(),\n","        geometry=basinSimple,\n","        scale=spatialScale,\n","        maxPixels=1e13,\n","        bestEffort=True,\n","        tileScale=tileScale\n","    )\n","    eviMeanFloodedOutput = ee.Number(eviMeanFlooded.get('EVI_mean'))\n","\n","    # Calculate mean EVI in dry forests\n","    eviMeanNonFlooded = evi_nonflooded.reduceRegion(\n","        reducer=ee.Reducer.mean(),\n","        geometry=basinSimple,\n","        scale=spatialScale,\n","        maxPixels=1e13,\n","        bestEffort=True,\n","        tileScale=tileScale\n","    )\n","    eviMeanNonFloodedOutput = ee.Number(eviMeanNonFlooded.get('EVI_mean'))\n","\n","    # Return feature with results\n","    return ee.Feature(None, {\n","        'flooded_area_km2_classified': areaValueclass,\n","        'EVI_mean_forests': eviMeanForestsOutput,\n","        'EVI_mean_flooded_forests': eviMeanFloodedOutput,\n","        'EVI_mean_non_flooded_forests': eviMeanNonFloodedOutput,\n","        'year': year\n","    })"],"metadata":{"id":"q6OimkoyjVwd","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1749464018103,"user_tz":-60,"elapsed":19,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"e2700424-1388-4425-c6f6-0c6ff03175b2"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"markdown","source":["## Productivity (EVI) trends"],"metadata":{"id":"M_SNvePui5Jc"}},{"cell_type":"code","source":["################################\n","### productivity timeseries ####\n","################################\n","\n","# apply scaling factor to EVI\n","def processEVI(image):\n","    return image.select('EVI').multiply(0.0001).copyProperties(image, ['system:time_start'])\n","\n","# Process MODIS EVI collection and return annual mean, min, and max without valid pixel masking\n","def process_evi_collection(evi_collection, start_year=2001, end_year=2024):\n","    # Create list of years\n","    years = ee.List.sequence(start_year, end_year)\n","    def process_annual_evi(year):\n","        year = ee.Number(year)\n","        start_date = ee.Date.fromYMD(year, 1, 1)\n","        end_date = start_date.advance(1, 'year')\n","        # Filter the EVI collection for April, May, June\n","        filtered_evi = evi_collection.filterDate(start_date, end_date).filter(ee.Filter.calendarRange(4, 6, 'month'))  # April, May, June\n","        # calculate total number of images for valid pixel masking\n","        total_images = ee.Number(filtered_evi.size())\n","        # Function to count valid (non-null) data in each pixel\n","        def count_valid_pixels(image):\n","            # Create a binary mask for valid pixels (1 for valid, 0 for invalid)\n","            valid_mask = image.select(\"EVI\").mask()  # Mask where pixels are valid (non-null)\n","            return valid_mask\n","        # Map the function over the collection to get valid masks for each image\n","        valid_masks = filtered_evi.map(count_valid_pixels)\n","        # Sum the valid pixels across the collection (count how many images have data at each pixel)\n","        valid_pixel_counts = valid_masks.sum().rename(\"pixel_counts\")\n","        # Calculate mean, min, and max for the filtered images\n","        annual_mean = filtered_evi.mean().rename('EVI_mean')\n","        # annual_min = filtered_evi.min().rename('EVI_min')\n","        # annual_max = filtered_evi.max().rename('EVI_max')\n","        # Combine the mean, min, and max into a single image\n","        combined = annual_mean.addBands(valid_pixel_counts)\n","        # Set the year and time start properties\n","        return combined.set('year', year).set('system:time_start', start_date.millis())\n","    # Process the EVI collection for each year\n","    annual_evi = ee.ImageCollection(years.map(process_annual_evi))\n","    return annual_evi\n","\n","# Function to mask non-forest pixels\n","def mask_forest(image):\n","    return image.updateMask(forest)\n","\n","### remove low data coverage pixels (<80%)\n","def maskEVI(image):\n","  return image.updateMask(low_data_mask.eq(1)).clip(basin)\n","\n","## mask rivers from EVI layers\n","def maskRivers(image):\n","  evi_bands = image.select([\"EVI_mean\"]).updateMask(riversMask.eq(0))\n","  return image.addBands(evi_bands, overwrite=True)"],"metadata":{"id":"-3rv2WpDi_g1","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1749464018104,"user_tz":-60,"elapsed":11,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"8ab7023f-6aaa-4cea-a78d-f457b409a788"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"markdown","source":["## Wetland GHG emmissions"],"metadata":{"id":"mWfcf0paiYqk"}},{"cell_type":"code","source":["#################################\n","#### WTLs and GHG emissions #####\n","#################################\n","\n","### Predict flood depth and water table levels (WTLs) from flood extents and HAND data\n","def compute_flood_depth_and_wtl(image):\n","    # Cleaned water extent from classification\n","    water_extent_cleaned = image.select(\"flooded_classification_kernel\").eq(1)\n","    # Flood Depth Calculation (based on 90th percentile of HAND in flooded area)\n","    hand_masked = hand.updateMask(water_extent_cleaned)\n","    hand_90 = hand_masked.reduceRegion(\n","        reducer=ee.Reducer.percentile([90]),\n","        geometry=hand_masked.geometry(),\n","        scale=90,\n","        maxPixels=1e13\n","    ).get('hnd')\n","    max_hand_img = ee.Image.constant(hand_90)\n","    flood_depth = max_hand_img.subtract(hand).rename(\"flood_depth\")\n","    flood_area = flood_depth.updateMask(water_extent_cleaned)\n","    # Below-Ground Water Table Level (based on 10th percentile of HAND in unflooded max extent area)\n","    dry_area = max_flooding_extent_cleaned.updateMask(water_extent_cleaned.Not())\n","    hand_5 = hand.updateMask(dry_area).reduceRegion(\n","        reducer=ee.Reducer.percentile([10]),\n","        geometry=dry_area.geometry(),\n","        scale=90,\n","        maxPixels=1e13\n","    ).get('hnd')\n","    min_hand_img = ee.Image.constant(hand_5)\n","    below_ground_WTL = hand.subtract(min_hand_img).multiply(-1).rename(\"below_ground_WTL\") \\\n","        .updateMask(dry_area)\n","    # Combine both into one WTLs band\n","    combined_flood_depth = flood_area.blend(below_ground_WTL).rename(\"combined_flood_depth\")\n","    # Add results to the original image\n","    return image.addBands([flood_depth, below_ground_WTL, combined_flood_depth])\n","\n","### classify flood depths in aggregations for GHG emissions\n","def classify_flood_depth(image):\n","    flood_depth = image.select(\"combined_flood_depth\")\n","    # Create binary classification bands\n","    flood_depth_gt_40 = flood_depth.gt(0.4).rename(\"flood_depth_gt_40\")\n","    flood_depth_0_40 = flood_depth.gt(-0.05).And(flood_depth.lte(0.4)).rename(\"flood_depth_0_40\")\n","    flood_depth_30_0 = flood_depth.gt(-0.30).And(flood_depth.lte(-0.05)).rename(\"flood_depth_30_0\")\n","    flood_depth_50_30 = flood_depth.gt(-0.5).And(flood_depth.lte(-0.30)).rename(\"flood_depth_50_30\")\n","    flood_depth_70_50 = flood_depth.gt(-0.7).And(flood_depth.lte(-0.5)).rename(\"flood_depth_70_50\")\n","    flood_depth_lt_70 = flood_depth.lte(-0.7).rename(\"flood_depth_lt_70\")\n","    # Combine into a single classification band\n","    flood_depth_binned = (\n","        flood_depth_gt_40.multiply(1)\n","        .add(flood_depth_0_40.multiply(2))\n","        .add(flood_depth_30_0.multiply(3))\n","        .add(flood_depth_50_30.multiply(4))\n","        .add(flood_depth_70_50.multiply(5))\n","        .add(flood_depth_lt_70.multiply(6))\n","        .rename(\"flood_depth_class\")\n","    )\n","    # Add all new bands to original image\n","    return image.addBands([\n","        flood_depth_gt_40,\n","        flood_depth_0_40,\n","        flood_depth_30_0,\n","        flood_depth_50_30,\n","        flood_depth_70_50,\n","        flood_depth_lt_70,\n","        flood_depth_binned\n","    ])\n","\n","### Calculate area of each flood depth class\n","def getFloodedAreaByDepth(image):\n","    \"\"\"Compute flooded forest area by depth zones for a given PALSAR image.\"\"\"\n","    date = ee.Date(image.get('system:time_start'))\n","    year = date.format('YYYY')\n","\n","    # Dictionary of depth classes with band names\n","    depth_bands = {\n","        'area_km2_flood_depth_gt_40cm': 'flood_depth_gt_40',\n","        'area_km2_flood_depth_minus5_40cm': 'flood_depth_0_40',\n","        'area_km2_flood_depth_minus30_minus5cm': 'flood_depth_30_0',\n","        'area_km2_flood_depth_minus50_minus30cm': 'flood_depth_50_30',\n","        'area_km2_flood_depth_minus70_minus50cm': 'flood_depth_70_50',\n","        'area_km2_flood_depth_lt_minus70cm': 'flood_depth_lt_70'\n","    }\n","\n","    def computeArea(bandName):\n","        # Mask and calculate pixel area\n","        masked = image.select(bandName).eq(1)\n","        areaImage = ee.Image.pixelArea().updateMask(masked)\n","        return areaImage.reduceRegion(\n","            reducer=ee.Reducer.sum(),\n","            geometry=basinSimple,\n","            scale=spatialScale,\n","            maxPixels=1e13,\n","            bestEffort=True,\n","            tileScale=tileScale  # Try increasing this\n","        ).get('area')\n","\n","    # Map over the depth band dictionary\n","    areas = {key: ee.Number(computeArea(band)).divide(1e6) for key, band in depth_bands.items()}\n","\n","    # Return feature\n","    return ee.Feature(None, {**areas, 'year': year})"],"metadata":{"id":"SFryG2qeprKM","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1749464018104,"user_tz":-60,"elapsed":9,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"cca80f7d-4d04-41bf-a96e-545c95ca2070"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"markdown","source":["## Statistical analyses"],"metadata":{"id":"m1SF3o7WjfEU"}},{"cell_type":"code","source":["###############################\n","##### statistical analyses ####\n","###############################\n","\n","# Compute confidence intervals (CIs) for swamp forest extents\n","def compute_ci_mapped_area(row):\n","    mapped_area = row[\"flooded_area_km2_classified\"]\n","    se = mapped_area * se_ua / ua  # Propagate uncertainty\n","    lower = max(mapped_area - 1.96 * se, 0)\n","    upper = mapped_area + 1.96 * se\n","    return pd.Series([lower, upper], index=[\n","        \"lower_95ci_km2\", \"upper_95ci_km2\"\n","    ])\n","\n","# Function to compute slope, 95% CI, and p-value for each subbasin swamp forest extent trend\n","def compute_slope_ci(group):\n","    X = group[['year']].values\n","    y = group['flooded_area_km2_classified'].values\n","    n = len(y)\n","\n","    if n < 3:\n","        return pd.Series({'slope': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan, 'p_value': np.nan})\n","\n","    model = LinearRegression()\n","    model.fit(X, y)\n","    y_pred = model.predict(X)\n","    residuals = y - y_pred\n","    df_deg = n - 2\n","    x_mean = np.mean(X)\n","\n","    # Standard error of the slope\n","    SE_slope = np.sqrt(np.sum(residuals**2) / df_deg) / np.sqrt(np.sum((X[:,0] - x_mean)**2))\n","\n","    # t-statistic for the slope\n","    slope = model.coef_[0]\n","    t_stat = slope / SE_slope\n","\n","    # Two-tailed p-value from t-distribution\n","    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df_deg))\n","\n","    # 95% Confidence Interval\n","    t_crit = stats.t.ppf(0.975, df_deg)\n","    ci_lower = slope - t_crit * SE_slope\n","    ci_upper = slope + t_crit * SE_slope\n","\n","    return pd.Series({\n","        'slope': slope,\n","        'ci_lower': ci_lower,\n","        'ci_upper': ci_upper,\n","        'p_value': p_value\n","    })"],"metadata":{"id":"Xw7relFNjjud","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1749464018106,"user_tz":-60,"elapsed":2,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"073f47df-7432-4220-b1a1-089b51420ac0"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"markdown","source":["# Swamp forest classification"],"metadata":{"id":"L6FJlEwEkzfh"}},{"cell_type":"code","source":["### load groundtruth coordinates and convert to GEE feature collection\n","\n","# load data\n","ground_df = pd.read_csv(f'{GoogleDrive_folder_path}/Crezee_2022_Nature_Ground_truth_datapoints.csv')\n","# filter to only peat swamps and dry forest\n","ground_df_filtered = ground_df[ground_df[\"Landcover_classification\"].isin([\n","    'Hardwood peat swamp forest', 'Palm peat swamp forest', 'Non-peat forming forest'\n","])].copy()\n","\n","# Aggregate peat swamp classes\n","ground_df_filtered[\"Landcover_classification\"] = ground_df_filtered[\"Landcover_classification\"].replace(\n","    ['Hardwood peat swamp forest', 'Palm peat swamp forest'],\n","    'Peat swamp forest'\n",")\n","# Convert the pandas dataframe to EE FeatureCollection\n","ee_fc = df_to_ee_feature_collection(ground_df_filtered)\n","\n","# Ensure features have a stable 'id' BEFORE applying randomColumn\n","ee_fc = ee_fc.sort('id')\n","# add integers for classes\n","ee_fc = ee_fc.map(add_class_property)\n","# Split into training (70%) and testing (30%)\n","ee_fc = ee_fc.randomColumn('random', seed=0)  # fixed seed\n","training = ee_fc.filter(ee.Filter.lt('random', 0.7)).sort('id')\n","testing = ee_fc.filter(ee.Filter.gte('random', 0.7)).sort('id')\n","\n","# Use the PALSAR-2 collection for 2019-2020 (ground sampling season)\n","PALSARImageCollection = PALSARCollection.filterDate(startDateImage, endDateImage)\n","# Apply preprocessing to the PALSAR collection\n","processedPalsarImage = PALSARImageCollection.map(applyBitmaskPalsar2).map(leeFilter).map(preprocessPalsarPalsar2).map(correctIncidenceAnglePalsar2)\n","# create annual image for the basin\n","PALSARimage = processedPalsarImage.median().clip(basin)"],"metadata":{"id":"mZ4S6Sy6k4B6","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1749464024400,"user_tz":-60,"elapsed":162,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"c12c45e3-c0c9-4ec5-bbf6-f441101c8919"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### Train classifier on PALSAR-2 2019 image\n","\n","# Set HAND thresholds and combine with forest mask to identify potential areas.\n","HANDlt20 = HAND.select('hnd').lt(20)\n","HAND0 = HAND.select('hnd').lte(0.1)\n","HANDlt20binary = HANDlt20.eq(1)\n","HAND0binary = HAND0.selfMask()\n","# Ensure forest is binary\n","forestBinary = forest.select('Map').eq(1)\n","forestBinary20 = forestBinary.updateMask(HANDlt20)\n","# Create flood potential mask by multiplying HAND binary mask with forest mask.\n","flooded20 = HANDlt20binary.multiply(forestBinary20)\n","# Create a ratio image to include HH/HV\n","ratio = PALSARimage.addBands(\n","    PALSARimage.select('HV_corrected_db')\n","    .divide(PALSARimage.select('HH_corrected_db'))\n","    .rename('HV_HH_ratio')\n",")\n","# Define DEM threshold to exclude high ground - Similar to Crezee 2022 paper\n","DEMthreshold = meritDEM.select('dem').lte(600)\n","# Add HAND band to the ratio image\n","combined_image = ratio.addBands(HAND.select('hnd'))\n","# Sample the combined image at training points\n","training_data = combined_image.sampleRegions(\n","    collection=training,\n","    properties=['class'],\n","    scale=25  # PALSAR resolution\n",")\n","# Train the random forest classifier with all bands\n","classifier = ee.Classifier.smileRandomForest(numberOfTrees=500, seed=0).train(\n","    features=training_data,\n","    classProperty='class',\n","    inputProperties=['HH_corrected_db', 'HV_corrected_db', 'HV_HH_ratio', 'hnd']\n",")\n","# Classify the combined image within the masked areas\n","image_to_classify = combined_image.updateMask(flooded20).updateMask(DEMthreshold)\n","classified = image_to_classify.classify(classifier)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"rwiSTUMIpjjQ","executionInfo":{"status":"ok","timestamp":1749464027956,"user_tz":-60,"elapsed":6,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"709b9a01-22f7-4247-a692-cdabe269a778"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### Classification accuracy assessment\n","# the classification can vary slightly due to upstream changes in GEE data and functions - this can cause minor changes to subsequent results\n","\n","# Sample the image at the testing points to get band values and actual class\n","testing_data = combined_image.sampleRegions(\n","    collection=testing,\n","    properties=['class'],\n","    scale=25\n",")\n","# Classify the testing data using the trained classifier\n","classified_testing = testing_data.classify(classifier)\n","# Compute the confusion matrix by comparing actual and predicted classes\n","confusion_matrix = classified_testing.errorMatrix('class', 'classification')\n","# Retrieve the confusion matrix as a 2D list\n","cm = confusion_matrix.getInfo()\n","# Compute accuracy metrics using Earth Engine methods\n","overall_accuracy = confusion_matrix.accuracy().getInfo()           # Overall accuracy\n","producers_accuracy = confusion_matrix.producersAccuracy().getInfo() # Producer's accuracy per class\n","users_accuracy = confusion_matrix.consumersAccuracy().getInfo()     # User's accuracy per class\n","kappa = confusion_matrix.kappa().getInfo()                         # Kappa coefficient\n","# Print accuracy metrics with two decimal places\n","print(\"\\nAccuracy Metrics:\")\n","print(f\"Overall Accuracy: {overall_accuracy:.2f}\")\n","print(f\"Producer's Accuracy for Non-peat swamp forest (0): {producers_accuracy[0][0]:.2f}\")\n","print(f\"Producer's Accuracy for Peat swamp forest (1): {producers_accuracy[1][0]:.2f}\")\n","print(f\"User's Accuracy for Non-peat swamp forest (0): {users_accuracy[0][0]:.2f}\")\n","print(f\"User's Accuracy for Peat swamp forest (1): {users_accuracy[0][1]:.2f}\")"],"metadata":{"id":"b4t6VgYFtZlM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Apply classification to annual timeseries\n","\n","## Create a list of annual intervals for PALSAR-2 and PALSAR-1 time periods\n","# PALSAR-1 - 2007-2010\n","nIntervalsPalsar1 = endDatePalsar1.difference(startDatePalsar1, 'year').floor()\n","intervalsPalsar1 = ee.List.sequence(0, nIntervalsPalsar1.subtract(1)).map(\n","    lambda i: ee.Dictionary({\n","        'start': startDatePalsar1.advance(ee.Number(i), 'year').format('YYYY-MM-dd'),\n","        'end': startDatePalsar1.advance(ee.Number(i), 'year').advance(1, 'year').format('YYYY-MM-dd')\n","    })\n",")\n","# PALSAR-2 - 2015-2024\n","nIntervalsPalsar2 = endDatePalsar2.difference(startDatePalsar2, 'year').floor()\n","intervalsPalsar2 = ee.List.sequence(0, nIntervalsPalsar2.subtract(1)).map(\n","    lambda i: ee.Dictionary({\n","        'start': startDatePalsar2.advance(ee.Number(i), 'year').format('YYYY-MM-dd'),\n","        'end': startDatePalsar2.advance(ee.Number(i), 'year').advance(1, 'year').format('YYYY-MM-dd')\n","    })\n",")\n","\n","# Create image collections by mapping the processing function over the intervals for PALSAR-1 and PALSAR-2\n","palsar1 = ee.ImageCollection(intervalsPalsar1.map(getFloodedImageFromMosaic))\n","palsar2 = ee.ImageCollection(intervalsPalsar2.map(getFloodedImage))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1749464235142,"user_tz":-60,"elapsed":340,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"efbd9d7c-ea9d-4d5f-ed2e-0761e7b0787d","id":"1aK6sqyRNEof"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### Export classified images as a GEE image collection for further analyses - required to break up processing chain and prevent exceeding memory use limits\n","# run time ~ 15 mins - export time ~ 1.5 hours\n","\n","# select only the required bands\n","PALSAR1_collection_cleaned = palsar1.map(select_bands_PALSAR1) # include no data mask band - not required for PALSAR-2\n","PALSAR2_collection_cleaned = palsar2.map(select_bands_PALSAR2)\n","\n","# Iterate through the collection and export each image as an asset - seperate for PALSAR1 and PALSAR2 due to additional no data band in PALSAR1\n","# PALSAR-1\n","PALSAR1_image_list = PALSAR1_collection_cleaned.toList(PALSAR1_collection_cleaned.size())\n","for i in range(PALSAR1_collection_cleaned.size().getInfo()):\n","    image = ee.Image(PALSAR1_image_list.get(i))\n","    save_PALSAR1_to_asset(image, i)\n","# PALSAR-2\n","PALSAR2_image_list = PALSAR2_collection_cleaned.toList(PALSAR2_collection_cleaned.size())\n","for i in range(PALSAR2_collection_cleaned.size().getInfo()):\n","    image = ee.Image(PALSAR2_image_list.get(i))\n","    save_PALSAR2_to_asset(image, i)"],"metadata":{"id":"9F63B9lD-_VR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Process EVI data"],"metadata":{"id":"88ncJ2Lp8ElO"}},{"cell_type":"code","source":["### process EVI to get annual images with only pixels with >80% data coverage in whole year- improve low data masking\n","\n","# Load MODIS EVI data\n","evi = modis \\\n","    .filterBounds(basin) \\\n","    .filterDate('2007-01-01', '2025-01-01') \\\n","    .map(maskModisQA) \\\n","    .map(processEVI) \\\n","    .map(mask_forest) \\\n","    #.select('EVI')\n","# apply filter to select only April-May-June images per year and calculate EVI means\n","eviSmoothed = process_evi_collection(evi, 2007, 2024)\n","# Remove low data pixels\n","total_pixel_counts = eviSmoothed.select(\"pixel_counts\").sum()\n","total_images_threshold = ee.Number(54).multiply(0.8) #54 for 3 month periods, 216 for all months\n","low_data_mask = total_pixel_counts.gte(total_images_threshold)\n","eviSmoothedMasked = eviSmoothed.map(maskEVI)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1749464304768,"user_tz":-60,"elapsed":9,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"ff79f691-a20f-455b-d395-5937020fbbf2","id":"EL3yGJCoMSKx"},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### Export EVI image collection as GEE asset for further analyses\n","# run time ~ 3 mins - export time ~ 30 mins (accumlates with PALSAR export)\n","EVI_sorted_collection = eviSmoothedMasked.sort('system:time_start')\n","# Select bands\n","EVI_collection_cleaned = EVI_sorted_collection.map(select_bands_EVI)\n","# Iterate through the collection and export each image as an asset\n","EVI_image_list = EVI_collection_cleaned.toList(EVI_collection_cleaned.size())\n","for i in range(EVI_collection_cleaned.size().getInfo()):\n","    image = ee.Image(EVI_image_list.get(i))\n","    save_EVI_to_asset(image, i)"],"metadata":{"id":"gTG-Y5M8_GHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Swamp forest and productivity trends\n","Run this section after previous exports are finished. Check progress in Google Earth Engine Task Manager."],"metadata":{"id":"WD21U2ZsvoNm"}},{"cell_type":"code","source":["### Load data\n","\n","# # swamp forest classifications\n","flooded_forests = ee.ImageCollection(asset_path_PALSAR_loadData) # update path to your collection\n","\n","# # evi\n","evi = ee.ImageCollection(asset_path_EVI_loadData) # update path to your collection\n","\n","# remove evi images without flood match up years for further analyses - Keep only images from 2007–2010 and 2015–2024\n","evi = ee.ImageCollection(evi.filterDate('2007-01-01', '2011-01-01')) \\\n","        .merge(evi.filterDate('2015-01-01', '2025-01-01'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"27IW3WXco6z4","executionInfo":{"status":"ok","timestamp":1749464361202,"user_tz":-60,"elapsed":3,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"52be4aba-0575-4d2b-c97a-4766be811ad3"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### merge swamp forest extents with EVI images collections for productivity trend analyses in wet and dry ecosystems\n","\n","# Filter the ImageCollections by date to ensure consistency\n","flooded_forests_filtered = flooded_forests.filterDate(\"2007-01-01\", \"2025-01-01\")\n","evi_filtered = evi.filterDate(\"2007-01-01\", \"2025-01-01\")\n","# Define an inner join\n","inner_join = ee.Join.inner()\n","# Specify an equals filter for image timestamps\n","filter_time_eq = ee.Filter.equals(\n","    leftField='system:time_start', rightField='system:time_start'\n",")\n","# Apply the join\n","inner_joined_modis = inner_join.apply(evi_filtered, flooded_forests_filtered, filter_time_eq)\n","# Merge the results in the output FeatureCollection\n","merged_collection = inner_joined_modis.map(\n","    lambda feature: ee.Image.cat(\n","        feature.get('primary'), feature.get('secondary')\n","    )\n",")\n","# Ensure output is a GEE image collection\n","merged_collection = ee.ImageCollection(merged_collection)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"BbVW3BUuk2hm","executionInfo":{"status":"ok","timestamp":1749464364059,"user_tz":-60,"elapsed":11,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"04f36614-70f1-4695-ffda-2e01baaf6a9e"},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### Fill data gaps in 2008 flood classifcation image and apply functions to smooth/mask low quality data\n","\n","# Select the 2008 and 2007 images\n","PALSARimage2008 = merged_collection.filterMetadata('year', 'equals', 2008).first()\n","PALSARimage2007 = merged_collection.filterMetadata('year', 'equals', 2007).first()\n","# Select the flooded_classification band from both images\n","floodedClass2008 = PALSARimage2008.select('flooded_classification')\n","floodedClass2007 = PALSARimage2007.select('flooded_classification')\n","# Select the no_data_mask band from the 2008 image\n","noDataMask = PALSARimage2008.select('no_data_mask')\n","# Replace missing values in the 2008 image's flooded_classification band using the 2007 values\n","floodedClass2008Filled = floodedClass2008.where(noDataMask.eq(1), floodedClass2007)\n","PALSARimage2008Updated = PALSARimage2008.addBands(\n","    floodedClass2008Filled.rename('flooded_classification'),\n","    overwrite=True\n",")\n","# Apply the data gap filling function to the PALSAR 2008 image\n","merged_collection = merged_collection.map(Fill_data_gaps_PALSAR1)\n","# # apply smoothing to remove isolated flood patch pixels for GHG emissions analyses\n","merged_collection = merged_collection.map(kernelFilter)\n","# # mask rivers from EVI bands\n","merged_collection = merged_collection.map(maskRivers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"1iF9UftM_c2m","executionInfo":{"status":"ok","timestamp":1749464366285,"user_tz":-60,"elapsed":9,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"8b7cace6-8316-4bb4-cd3e-ce6cadc4db51"},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### Export swamp forest extent trends per subbasin\n","\n","# select only subbasins in the forest region\n","subbasins_forest = subbasins.filterBounds(forest.geometry())\n","# export swamp extent trends by subbasin\n","areaFeatureLists = merged_collection.map(getFloodedAreaSubbasins)\n","flattened = areaFeatureLists.flatten()\n","# Export to Drive\n","task = ee.batch.Export.table.toDrive(\n","    collection=flattened,\n","    description='Annual_Swamp_forest_extents_by_subbasin',\n","    folder=GoogleDrive_folder,\n","    fileFormat='CSV'\n",")\n","task.start()\n","print(\"Export task started (estimated processing time = 25min). Check Google Drive for the CSV file.\")"],"metadata":{"id":"MkbmxA67ZGod"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Identify pixels with permanent flooding, max flood extent, flood loss and flood gain - compares palsar-1 period with palsar-2 period\n","\n","# Get the list of images sorted by time\n","sorted_images = merged_collection.sort(\"system:time_start\").select(\"flooded_classification_kernel\") # use smoothed flood layer\n","# Split into early (first 4 images - PALSAR-1) and later (remaining images - PALSAR-2)\n","early_period = ee.ImageCollection(sorted_images.toList(4))  # First 4 images\n","later_period = ee.ImageCollection(sorted_images.toList(sorted_images.size()).slice(4))  # Rest of the images\n","# Count the number of flooded years in early and later periods\n","early_flooded_count = early_period.sum()\n","later_flooded_count = later_period.sum()\n","# identify flood loss and gain\n","flooded_loss = early_flooded_count.gt(0).And(later_flooded_count.eq(0)).selfMask()\n","flooded_gain = early_flooded_count.eq(0).And(later_flooded_count.gt(0)).selfMask()\n","# identify permanent flooding and dry forests\n","always_flooded = sorted_images.sum().gte(14).selfMask()\n","# Only mask the never_flooded class where forest is present — don’t let it propagate to other classes\n","never_flooded = sorted_images.sum().eq(0).And(forest).selfMask()\n","# max flood extent for GHG calculations\n","max_flood_extent = sorted_images.sum().gt(0).And(sorted_images.sum().lte(14))  # Between 1 and 11"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"E6WyBvrTH4Qh","executionInfo":{"status":"ok","timestamp":1749462523831,"user_tz":-60,"elapsed":10,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"92cc0ddd-0a05-4eb7-cd48-5ddf70bbd706"},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### Export trends in swamp forest extents, EVI in swamp forests and EVI in dry forests\n","\n","# Define dry forests for following function\n","dry_forests = never_flooded\n","# Map the function over the PALSAR image collection\n","FloodAndEVIFeatures = merged_collection.map(getFloodedAreaAndEVI)\n","# Export the results to Google Drive as CSV\n","task = ee.batch.Export.table.toDrive(\n","    collection=FloodAndEVIFeatures,\n","    description='Summary_stats_SwampForestExtents_and_EVI_2007_2024',\n","    folder=GoogleDrive_folder,\n","    fileFormat='CSV'\n",")\n","task.start()\n","print(\"Export task started (estimated processing time = 20 mins). Check Google Drive for the CSV file.\")"],"metadata":{"id":"dZB8uINWOE5q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Export EVI image collection as stacked image for pixel-wise trend analyses\n","\n","# Sort chronologically\n","evi_to_export = merged_collection.sort(\"year\")\n","# Rename bands to EVI_YEAR\n","def rename_bands(img):\n","    year = ee.Number(img.get(\"year\")).format('%d')  # force int and convert to string\n","    return img.select(\"EVI_mean\").rename(ee.String(\"EVI_\").cat(year))\n","# Create a band stack\n","evi_stack = evi_to_export.map(rename_bands).toBands().clip(basin)\n","# Export to drive\n","task = ee.batch.Export.image.toDrive(\n","    image=evi_stack,\n","    description='EVI_Annual_Stack',\n","    folder=GoogleDrive_folder,\n","    fileNamePrefix='EVI_2007_2024_Stack',\n","    region=basin,\n","    scale=1000,\n","    maxPixels=1e13,\n","    fileFormat='GeoTIFF'\n",")\n","task.start()\n","print(\"Export task started (estimated processing time = 6 mins). Check Google Drive for the CSV file.\")"],"metadata":{"id":"UVnruMNMQJX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Flood depths and Water Table Levels (WTLs)"],"metadata":{"id":"PBdWYOKzvrZ5"}},{"cell_type":"code","source":["### load data and classify WTLs\n","\n","# filter hand data to clean flood depth layers to less than 20 hand (potential flooding area)\n","hand = HAND.select('hnd')  # Assuming the HAND band is named 'hnd'\n","hand_lt20 = hand.lt(20)\n","# max flooding extent over study period excluding non-flood risk pixels\n","max_flooding_extent = max_flood_extent\n","max_flooding_extent_cleaned = max_flooding_extent.updateMask(hand_lt20)\n","# select the cleaned flood classification layer\n","merged_collection_flooding = merged_collection.select(\"flooded_classification_kernel\")\n","## predict flood depths and WTLs\n","WaterTableLevels = merged_collection_flooding.map(compute_flood_depth_and_wtl)\n","## aggregate WTLs into groupings for GHG emissions\n","WTL_classes = WaterTableLevels.map(classify_flood_depth)\n","# Calculate area of each WTL class\n","WTLareaFeatures = WTL_classes.map(getFloodedAreaByDepth)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"lpNZ64_3eH20","executionInfo":{"status":"ok","timestamp":1749462600242,"user_tz":-60,"elapsed":31,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"6d86f218-6e19-4998-a7f2-5010922fd537"},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["# Export WTLs and flood depth areas to Google Drive as CSV\n","task = ee.batch.Export.table.toDrive(\n","    collection=WTLareaFeatures,\n","    description='Summary_stats_Flood_depths_and_WTLs_areas_2007_2024',\n","    folder=GoogleDrive_folder,\n","    fileFormat='CSV'\n",")\n","task.start()\n","print(\"Export task started. Check Google Drive for the CSV file.\")"],"metadata":{"id":"R3aUE-6Ff4v8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Statistical analyses\n","Run after all exports have finished. Check progress in the Google Earth Engine task manager."],"metadata":{"id":"um6m6W3IvwVg"}},{"cell_type":"markdown","source":["## Swamp forest extent trends"],"metadata":{"id":"DCs1SPPIcoy7"}},{"cell_type":"code","source":["### load data\n","area_trends = pd.read_csv(f'{GoogleDrive_folder_path}/Summary_stats_SwampForestExtents_and_EVI_2007_2024.csv')\n","# select only flood area and year\n","area_trends = area_trends[[\"year\", \"flooded_area_km2_classified\"]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"wkTEQ3JLGQjk","executionInfo":{"status":"ok","timestamp":1749464466974,"user_tz":-60,"elapsed":44,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"556ed2c9-2333-485f-c7f5-0ef2bf571426"},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### confidence intervals of wetland extent\n","\n","# Accuracy metrics\n","ua = 0.80  # User's Accuracy\n","# Number of testing samples\n","n = 326\n","# Compute standard error of User’s Accuracy\n","se_ua = math.sqrt(ua * (1 - ua) / n)\n","# Apply the function to each row\n","area_trends_ci = area_trends.copy()\n","area_trends_ci[[\"lower_95ci_km2\", \"upper_95ci_km2\"]] = area_trends_ci.apply(compute_ci_mapped_area, axis=1)\n","\n","# print output - annual Congo Basin swamp forest extents with confidence intervals\n","area_trends_ci"],"metadata":{"id":"KMxoX997FUG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### plot change in swamp forest extent\n","\n","# Create a single figure and axis\n","fig, ax = plt.subplots(figsize=(14, 3))\n","\n","# Define a function to add a linear trend line and annotate slope + p-value\n","def add_trend_line(x, y, ax, color='k'):\n","    valid_data = pd.DataFrame({'x': x, 'y': y}).dropna()\n","    x_valid = valid_data['x']\n","    y_valid = valid_data['y']\n","\n","    if len(x_valid) > 1:\n","        slope, intercept, r_value, p_value, std_err = linregress(x_valid, y_valid)\n","        trend_line = slope * x_valid + intercept\n","        ax.plot(x_valid, trend_line, color=color, linestyle='--')\n","\n","        ax.text(0.01, 0.95,\n","                f\"Slope: {slope:.2f}\\nP-value: {p_value:.3f}\",\n","                transform=ax.transAxes,\n","                fontsize=16,\n","                verticalalignment='top',\n","                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n","\n","# Plot flooded_area_km2_classified\n","ax.plot(area_trends_ci['year'], area_trends_ci['flooded_area_km2_classified'], color='b', marker='o', linestyle='-')\n","ax.set_ylabel('Flooding (km²)', fontsize=16)\n","add_trend_line(area_trends_ci['year'], area_trends_ci['flooded_area_km2_classified'], ax, color='b')\n","\n","# Set x-axis label and ticks\n","ax.set_xlabel('Year', fontsize=16)\n","ax.set_xticks([2010, 2015, 2020])\n","ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}'))\n","ax.tick_params(axis='both', labelsize=16)\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"Or8aLnWieVGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### load subbasins data\n","subbasins_trends = pd.read_csv(f'{GoogleDrive_folder_path}/Annual_Swamp_forest_extents_by_subbasin.csv')\n","\n","# convert area to km2 and select only required columns\n","subbasins_trends[\"flooded_area_km2_classified\"] = subbasins_trends[\"sum\"] / 1e6\n","subbasins_trends = subbasins_trends[[\"year\", \"HYBAS_ID\", \"flooded_area_km2_classified\"]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"vyso-3qx9TZm","executionInfo":{"status":"ok","timestamp":1749464483181,"user_tz":-60,"elapsed":535,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"48b078fc-d3b7-4003-fb72-ce5040f65623"},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["# Apply slope calculation with CIs per subbasin\n","results = subbasins_trends.groupby('HYBAS_ID').apply(compute_slope_ci).reset_index()\n","# View results\n","results"],"metadata":{"id":"u84XmIg0-EDM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Forest productivity trends"],"metadata":{"id":"gpwVGqifcwPS"}},{"cell_type":"code","source":["### EVI pixel-wise significant trends\n","with rasterio.open(f'{GoogleDrive_folder_path}/EVI_2007_2024_Stack.tif') as src:\n","    stack = src.read()  # shape: (years, rows, cols)\n","    profile = src.profile\n","rows, cols = stack.shape[1], stack.shape[2]\n","years = np.arange(2007, 2007 + stack.shape[0])  # adjust start year\n","slope_arr = np.full((rows, cols), np.nan)\n","pval_arr = np.full((rows, cols), np.nan)\n","for i in range(rows):\n","    for j in range(cols):\n","        y = stack[:, i, j]\n","        if np.any(np.isnan(y)) or np.all(y == 0):  # skip missing or empty pixels\n","            continue\n","        slope, _, _, p_value, _ = linregress(years, y)\n","        slope_arr[i, j] = slope\n","        pval_arr[i, j] = p_value"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"W9x7iyWkT3ZB","executionInfo":{"status":"ok","timestamp":1749462943262,"user_tz":-60,"elapsed":246294,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"98110da1-c115-4360-c162-877ca737b430"},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### plot slopes\n","plt.figure(figsize=(10, 6))\n","plt.imshow(np.flipud(slope_arr), cmap='RdYlGn', vmin=-0.01, vmax=0.01)\n","plt.title(\"EVI Trend (Slope per Pixel)\")\n","plt.colorbar(label='Slope')\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"B6xi2rGmnzxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### print percentage of pixels with significant trends\n","\n","# Flatten arrays to make pixel-wise calculations easier\n","slope_flat = slope_arr.flatten()\n","pval_flat = pval_arr.flatten()\n","# Mask out nan or 0 pixels\n","valid_mask = ~np.isnan(slope_flat) & (slope_flat != 0)\n","# Apply valid mask\n","slope_flat = slope_flat[valid_mask]\n","pval_flat = pval_flat[valid_mask]\n","# Total valid pixels\n","total = len(slope_flat)\n","# Conditions\n","increasing = slope_flat > 0\n","decreasing = slope_flat < 0\n","significant = pval_flat < 0.05\n","# Calculate each category\n","inc_total = np.sum(increasing)\n","dec_total = np.sum(decreasing)\n","inc_sig = np.sum(increasing & significant)\n","dec_sig = np.sum(decreasing & significant)\n","# Percentages\n","print(f\"Total valid pixels: {total}\")\n","print(f\"Increasing trend: {inc_total} pixels ({(inc_total / total) * 100:.2f}%)\")\n","print(f\"Significant increasing trend: {inc_sig} pixels ({(inc_sig / total) * 100:.2f}%)\")\n","print(f\"Decreasing trend: {dec_total} pixels ({(dec_total / total) * 100:.2f}%)\")\n","print(f\"Significant decreasing trend: {dec_sig} pixels ({(dec_sig / total) * 100:.2f}%)\")"],"metadata":{"id":"nEkiVh40qAY5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### load evi trends data\n","evi_trends = pd.read_csv(f'{GoogleDrive_folder_path}/Summary_stats_SwampForestExtents_and_EVI_2007_2024.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"A3yM5W4Rauar","executionInfo":{"status":"ok","timestamp":1749463558649,"user_tz":-60,"elapsed":15,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"7bdb91fe-5456-4638-8481-fd0a4f5735a2"},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### Congo Basin timeseries trends for EVI in swamp forests, dry forests and all forests\n","\n","# Create a single figure and axis\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Updated function to add trend line and label it with custom name and matching color\n","def add_trend_line(x, y, ax, color='k', label_pos=0.95, series_name=''):\n","    valid_data = pd.DataFrame({'x': x, 'y': y}).dropna()\n","    x_valid = valid_data['x']\n","    y_valid = valid_data['y']\n","\n","    if len(x_valid) > 1:\n","        slope, intercept, r_value, p_value, std_err = linregress(x_valid, y_valid)\n","        trend_line = slope * x_valid + intercept\n","        ax.plot(x_valid, trend_line, color=color, linestyle='--')\n","        ax.text(0.01, label_pos,\n","                f\"Slope ({series_name}): {slope:.4f}, P: {p_value:.3f}\",\n","                color=color,  # <- Match text color to trend line\n","                transform=ax.transAxes,\n","                fontsize=14,\n","                verticalalignment='top',\n","                bbox=dict(facecolor='white', alpha=0.6, edgecolor='none'))\n","\n","# Plot function with added series_name\n","def plot_mean(ax, year, mean_col, color, label, trend_y, series_name):\n","    ax.plot(year, evi_trends[mean_col], color=color, linestyle='-', marker='o', label=f'{label} (mean)')\n","    add_trend_line(year, evi_trends[mean_col], ax, color=color, label_pos=trend_y, series_name=series_name)\n","\n","# Plot all three with custom names\n","plot_mean(ax, evi_trends['year'],\n","                  'EVI_mean_flooded_forests',\n","                  'blue', 'Flooded forest EVI', trend_y=0.95, series_name='Flooded')\n","\n","plot_mean(ax, evi_trends['year'],\n","                  'EVI_mean_non_flooded_forests',\n","                  'green', 'Terra firme EVI', trend_y=0.90, series_name='Terra firme')\n","\n","plot_mean(ax, evi_trends['year'],\n","                  'EVI_mean_forests',\n","                  'black', 'All forests EVI', trend_y=0.85, series_name='All forests')\n","\n","# Axis labels and formatting\n","ax.set_xlabel('Year', fontsize=18)\n","ax.set_ylabel('EVI', fontsize=18)\n","ax.set_xticks([2010, 2015, 2020])\n","ax.tick_params(axis='both', labelsize=18)\n","ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}'))\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"81OXA39DZuKb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run paired t-test\n","t_stat, p_val = ttest_rel(evi_trends[\"EVI_mean_flooded_forests\"], evi_trends[\"EVI_mean_non_flooded_forests\"])\n","print(f\"Paired t-test:\\n  t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}\")\n","# Non-parametric alternative\n","w_stat, p_wilcox = wilcoxon(evi_trends[\"EVI_mean_flooded_forests\"], evi_trends[\"EVI_mean_non_flooded_forests\"])\n","print(f\"Wilcoxon signed-rank test:\\n  statistic = {w_stat:.4f}, p-value = {p_wilcox:.4f}\")"],"metadata":{"id":"Z-pfLgmcbURd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GHG emissions"],"metadata":{"id":"HbZlneLIdCfU"}},{"cell_type":"code","source":["### Load data\n","\n","# Flood depths and WTLs\n","df = pd.read_csv(f'{GoogleDrive_folder_path}/Summary_stats_Flood_depths_and_WTLs_areas_2007_2024.csv')\n","# GHG fluxes by WTL\n","ghg = pd.read_csv(f'{GoogleDrive_folder_path}/Zou_2022_GHG_fluxes_by_WTL_tropics.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"UKAcf3dw6dAE","executionInfo":{"status":"ok","timestamp":1749464510329,"user_tz":-60,"elapsed":43,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"83d7c614-4a04-46c2-c46c-5984a6da7dbb"},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["# Identify columns that are in per hectare per year units\n","ha_yr_cols = [col for col in ghg.columns if '_ha_yr' in col]\n","# Create a new DataFrame by copying the original\n","ghg_km2 = ghg.copy()\n","# Convert ha-based values to km² by multiplying by 100\n","ghg_km2[ha_yr_cols] = ghg_km2[ha_yr_cols] * 100\n","\n","# Map WTL values to flood depth area column names\n","wtl_to_area_col = {\n","    -3: 'area_km2_flood_depth_lt_minus70cm',\n","    -2: 'area_km2_flood_depth_minus70_minus50cm',\n","    -1: 'area_km2_flood_depth_minus50_minus30cm',\n","     0: 'area_km2_flood_depth_minus30_minus5cm',\n","     1: 'area_km2_flood_depth_minus5_40cm',\n","     2: 'area_km2_flood_depth_gt_40cm'\n","}\n","# Select only relevant WTL rows from ghg_km2\n","ghg_selected = ghg_km2[ghg_km2['WTL'].isin(wtl_to_area_col.keys())]\n","# Create a list to store results\n","results = []\n","# Iterate over each row (year) in df\n","for _, row in df.iterrows():\n","    year = row['year']\n","    total_CO2 = total_CO2_lower = total_CO2_upper = 0\n","    total_CH4 = total_CH4_lower = total_CH4_upper = 0\n","    total_N2O = total_N2O_lower = total_N2O_upper = 0\n","    total_GHGs = total_GHGs_lower = total_GHGs_upper = 0\n","    # For each WTL class, multiply emissions by area\n","    for wtl, area_col in wtl_to_area_col.items():\n","        area = row[area_col]  # in km²\n","        emissions = ghg_selected[ghg_selected['WTL'] == wtl].iloc[0]\n","        total_CO2     += area * emissions['NEE_CO2_T_ha_yr']           # already per km²\n","        total_CO2_lower += area * emissions['Lower_95_CI_CO2_T_ha_yr']\n","        total_CO2_upper += area * emissions['Upper_95_CI_CO2_T_ha_yr']\n","        total_CH4     += area * emissions['CH4_KG_ha_yr']\n","        total_CH4_lower += area * emissions['Lower_95_CI_CH4_KG_ha_yr']\n","        total_CH4_upper += area * emissions['Upper_95_CI_CH4_KG_ha_yr']\n","        total_N2O     += area * emissions['N20_KG_ha_yr']\n","        total_N2O_lower += area * emissions['Lower_95_CI_N20_KG_ha_yr']\n","        total_N2O_upper += area * emissions['Upper_95_CI_N20_KG_ha_yr']\n","        total_GHGs     += area * emissions['SUM_GHGs_T_CO2eq_ha_yr']\n","        total_GHGs_lower += area * emissions['Lower_95_CI_SUM_GHGs_T_CO2eq_ha_yr']\n","        total_GHGs_upper += area * emissions['Upper_95_CI_SUM_GHGs_T_CO2eq_ha_yr']\n","    results.append({\n","        'year': year,\n","        'Total_CO2_T': total_CO2,\n","        'Lower_95_CI_CO2_T': total_CO2_lower,\n","        'Upper_95_CI_CO2_T': total_CO2_upper,\n","        'Total_CH4_KG': total_CH4,\n","        'Lower_95_CI_CH4_KG': total_CH4_lower,\n","        'Upper_95_CI_CH4_KG': total_CH4_upper,\n","        'Total_N2O_KG': total_N2O,\n","        'Lower_95_CI_N2O_KG': total_N2O_lower,\n","        'Upper_95_CI_N2O_KG': total_N2O_upper,\n","        'Total_GHGs_T_CO2eq': total_GHGs,\n","        'Lower_95_CI_GHGs_T_CO2eq': total_GHGs_lower,\n","        'Upper_95_CI_GHGs_T_CO2eq': total_GHGs_upper\n","    })\n","# Convert the results to a DataFrame\n","emissions_df = pd.DataFrame(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"3l19IcHKJE06","executionInfo":{"status":"ok","timestamp":1749464514357,"user_tz":-60,"elapsed":27,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"770ab28a-8df6-4221-aa7f-d3ec51eb216a"},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["# Convert CH4 and N2O from kg to metric tons (1 tonne = 1000 kg)\n","emissions_df['Total_CH4_T'] = emissions_df['Total_CH4_KG'] / 1000\n","emissions_df['Total_N2O_T'] = emissions_df['Total_N2O_KG'] / 1000\n","# Convert CH4 and N2O confidence intervals from kg to tonnes\n","emissions_df['Lower_95_CI_CH4_T'] = emissions_df['Lower_95_CI_CH4_KG'] / 1000\n","emissions_df['Upper_95_CI_CH4_T'] = emissions_df['Upper_95_CI_CH4_KG'] / 1000\n","emissions_df['Lower_95_CI_N2O_T'] = emissions_df['Lower_95_CI_N2O_KG'] / 1000\n","emissions_df['Upper_95_CI_N2O_T'] = emissions_df['Upper_95_CI_N2O_KG'] / 1000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"ut6i_TRGJ1dd","executionInfo":{"status":"ok","timestamp":1749464516605,"user_tz":-60,"elapsed":14,"user":{"displayName":"Aidan Byrne","userId":"16727349125498239788"}},"outputId":"1841fa0c-4b5b-47fa-96a1-aace4e8271eb"},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","            <style>\n","                .geemap-dark {\n","                    --jp-widgets-color: white;\n","                    --jp-widgets-label-color: white;\n","                    --jp-ui-font-color1: white;\n","                    --jp-layout-color2: #454545;\n","                    background-color: #383838;\n","                }\n","\n","                .geemap-dark .jupyter-button {\n","                    --jp-layout-color3: #383838;\n","                }\n","\n","                .geemap-colab {\n","                    background-color: var(--colab-primary-surface-color, white);\n","                }\n","\n","                .geemap-colab .jupyter-button {\n","                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n","                }\n","            </style>\n","            "]},"metadata":{}}]},{"cell_type":"code","source":["### remove kg columns to display\n","emissions_df = emissions_df.loc[:, ~emissions_df.columns.str.contains('KG')]\n","\n","pd.set_option('display.float_format', '{:.2f}'.format)\n","emissions_df"],"metadata":{"id":"6fCofRhLJGRh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine WTLs area with GHG fluxes per km2\n","\n","# Prepare individual data series\n","series_list = [\n","    (\"CO₂\", emissions_df['year'], emissions_df['Total_CO2_T'] / 1e8),\n","    (\"CH₄\", emissions_df['year'], emissions_df['Total_CH4_T'] / 1e7),\n","    (\"N₂O\", emissions_df['year'], emissions_df['Total_N2O_T'] / 1e6),\n","    (\"Total GHGs (CO₂eq)\", emissions_df['year'], emissions_df['Total_GHGs_T_CO2eq'] / 1e8)\n","]\n","# Function to compute stats\n","def print_trend_stats(label, x, y):\n","    valid_data = pd.DataFrame({'x': x, 'y': y}).dropna()\n","    x = valid_data['x']\n","    y = valid_data['y']\n","    if len(x) > 1:\n","        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n","        df = len(x) - 2\n","        t_crit = t.ppf(0.975, df)\n","        ci_lower = slope - t_crit * std_err\n","        ci_upper = slope + t_crit * std_err\n","        print(f\"{label} Trend:\")\n","        print(f\"  Slope: {slope:.5f}\")\n","        print(f\"  95% CI: ({ci_lower:.5f}, {ci_upper:.5f})\")\n","        print(f\"  P-value: {p_value:.5f}\\n\")\n","    else:\n","        print(f\"{label} Trend: Insufficient data\\n\")\n","# Apply to each series\n","for label, x, y in series_list:\n","    print_trend_stats(label, x, y)"],"metadata":{"id":"2uFT2aE-GDRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot GHG trends\n","\n","# Prepare individual data series\n","years = emissions_df['year']\n","co2 = emissions_df['Total_CO2_T']\n","ch4 = emissions_df['Total_CH4_T']\n","n2o = emissions_df['Total_N2O_T']\n","ghgs = emissions_df['Total_GHGs_T_CO2eq']\n","\n","# Create a figure with 4 subplots\n","fig, axes = plt.subplots(4, 1, figsize=(12, 12), sharex=True)\n","\n","# Function to add linear trend line and annotate slope & p-value\n","def add_trend_line(x, y, ax, color='k', x_pos=0.01, y_pos=0.95):\n","    valid_data = pd.DataFrame({'x': x, 'y': y}).dropna()\n","    x_valid = valid_data['x']\n","    y_valid = valid_data['y']\n","\n","    if len(x_valid) > 1:\n","        slope, intercept, r_value, p_value, std_err = linregress(x_valid, y_valid)\n","        trend_line = slope * x_valid + intercept\n","        ax.plot(x_valid, trend_line, color=color, linestyle='--')\n","\n","        # Add annotation\n","        ax.text(x_pos, y_pos,\n","                f\"Slope: {slope:.3f}\\nP-value: {p_value:.3f}\",\n","                transform=ax.transAxes,\n","                fontsize=16,\n","                verticalalignment='bottom' if y_pos < 0.5 else 'top',\n","                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n","\n","# CO2 – bottom left, scaled to 1e8\n","axes[0].plot(years, co2 / 1e8, color='forestgreen', marker='o', linestyle='-')\n","axes[0].set_ylabel('CO₂ (100 million tonnes)', fontsize=16)\n","add_trend_line(years, co2 / 1e8, axes[0], color='forestgreen', x_pos=0.01, y_pos=0.05)\n","axes[0].ticklabel_format(style='plain', axis='y')  # Disable scientific notation\n","\n","# CH4 – top left, scaled to 1e7\n","axes[1].plot(years, ch4 / 1e7, color='darkorange', marker='o', linestyle='-')\n","axes[1].set_ylabel('CH₄ (10 million tonnes)', fontsize=16)\n","add_trend_line(years, ch4 / 1e7, axes[1], color='darkorange', x_pos=0.01, y_pos=0.95)\n","axes[1].ticklabel_format(style='plain', axis='y')\n","\n","# N2O – bottom left, scale as needed\n","axes[2].plot(years, n2o / 1e6, color='royalblue', marker='o', linestyle='-')\n","axes[2].set_ylabel('N₂O (million tonnes)', fontsize=16)\n","add_trend_line(years, n2o / 1e6, axes[2], color='royalblue', x_pos=0.01, y_pos=0.05)\n","axes[2].ticklabel_format(style='plain', axis='y')\n","\n","# Total GHGs – bottom left, scaled to 1e8\n","axes[3].plot(years, ghgs / 1e8, color='black', marker='o', linestyle='-')\n","axes[3].set_ylabel('CO₂eq (100 million tonnes)', fontsize=16)\n","add_trend_line(years, ghgs / 1e8, axes[3], color='black', x_pos=0.01, y_pos=0.05)\n","axes[3].ticklabel_format(style='plain', axis='y')\n","\n","# X-axis formatting\n","axes[3].set_xlabel('Year', fontsize=16)\n","axes[3].set_xticks([2010, 2015, 2020])\n","\n","for ax in axes:\n","    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}'))\n","    ax.tick_params(axis='both', labelsize=14)\n","\n","plt.tight_layout()\n","\n","fig.align_ylabels(axes)\n","\n","plt.show()"],"metadata":{"id":"ICl4e3smnBQj"},"execution_count":null,"outputs":[]}]}